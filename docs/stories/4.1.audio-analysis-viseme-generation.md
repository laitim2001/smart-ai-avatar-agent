# Story 4.1: 音訊分析與 Viseme 數據生成(簡化版)

## Status
Draft

---

## Story

**As a** 開發者,
**I want** 分析 TTS 音訊產生 viseme 時間軸數據,
**so that** 可以驅動 Avatar 嘴型動畫。

---

## Acceptance Criteria

1. 建立 `lib/three/lipsync.ts`，實作音訊分析函式
2. 使用 Web Audio API `AnalyserNode` 分析音訊頻率
3. 實作簡化 viseme 映射表：高音量 → `aa`(張嘴)、中音量 → `E`(半開)、低音量 → `neutral`(閉嘴)
4. 產生 viseme 時間軸數據結構：`VisemeData[] = [{ time: number, viseme: string, weight: number }]`
5. 每 100ms 產生一個 viseme 數據點
6. 測試：輸入音訊檔，console.log 顯示 viseme 陣列
7. 目標準確度：70%(透過主觀視覺評估)

---

## Tasks / Subtasks

- [ ] **Task 1: 建立 Viseme 型別定義與數據結構** (AC: 4)
  - [ ] 建立目錄 `types/` (如尚未存在)
  - [ ] 建立 `types/lipsync.ts` 定義 Lip Sync 相關型別
  - [ ] 定義 VisemeData 介面：
    ```typescript
    /**
     * Viseme 數據點
     * 用於描述某個時間點應該顯示的嘴型
     */
    export interface VisemeData {
      /** 時間點（秒），相對於音訊開始時間 */
      time: number;
      /** Viseme 音素類型 */
      viseme: VisemeType;
      /** 嘴型權重（0-1），控制嘴型開合程度 */
      weight: number;
    }

    /**
     * Viseme 音素類型（簡化版）
     * POC 階段僅支援基礎音素分類
     */
    export type VisemeType = 'aa' | 'E' | 'I' | 'O' | 'U' | 'neutral';

    /**
     * 音訊分析配置
     */
    export interface AudioAnalysisConfig {
      /** FFT 大小（必須為 2 的冪次方） */
      fftSize: number;
      /** 取樣間隔（毫秒），每隔多久產生一個 viseme 數據點 */
      sampleInterval: number;
      /** 音量閾值配置 */
      volumeThresholds: {
        /** 高音量閾值（> 此值視為張嘴 aa） */
        high: number;
        /** 中音量閾值（> 此值視為半開 E） */
        medium: number;
        /** 低音量閾值（< 此值視為閉嘴 neutral） */
        low: number;
      };
    }
    ```
  - [ ] 定義預設配置常數：
    ```typescript
    /**
     * 預設音訊分析配置（POC 階段簡化版）
     */
    export const DEFAULT_AUDIO_ANALYSIS_CONFIG: AudioAnalysisConfig = {
      fftSize: 2048,              // FFT 大小，影響頻率解析度
      sampleInterval: 100,        // 100ms 取樣一次（符合 AC 要求）
      volumeThresholds: {
        high: 0.6,                // > 60% 音量視為張嘴
        medium: 0.3,              // > 30% 音量視為半開
        low: 0.1                  // < 10% 音量視為閉嘴
      }
    };
    ```
  - [ ] 加入 JSDoc 註解說明每個型別用途
  - [ ] 驗證型別定義無 TypeScript 錯誤

- [ ] **Task 2: 實作音訊分析基礎函式** (AC: 1, 2)
  - [ ] 建立目錄 `lib/three/` (如尚未存在)
  - [ ] 建立 `lib/three/lipsync.ts` Lip Sync 主函式檔
  - [ ] 實作 AudioContext 與 AnalyserNode 初始化：
    ```typescript
    import type { VisemeData, AudioAnalysisConfig } from '@/types/lipsync';
    import { DEFAULT_AUDIO_ANALYSIS_CONFIG } from '@/types/lipsync';

    /**
     * 音訊分析器類別
     * 負責分析音訊並產生 viseme 時間軸數據
     */
    export class AudioAnalyser {
      private audioContext: AudioContext;
      private analyser: AnalyserNode;
      private dataArray: Uint8Array;
      private config: AudioAnalysisConfig;

      constructor(config: Partial<AudioAnalysisConfig> = {}) {
        this.config = { ...DEFAULT_AUDIO_ANALYSIS_CONFIG, ...config };

        // 初始化 AudioContext
        this.audioContext = new AudioContext();

        // 建立 AnalyserNode
        this.analyser = this.audioContext.createAnalyser();
        this.analyser.fftSize = this.config.fftSize;

        // 建立數據陣列用於儲存頻率數據
        const bufferLength = this.analyser.frequencyBinCount;
        this.dataArray = new Uint8Array(bufferLength);
      }

      /**
       * 取得 AudioContext 實例（供外部使用）
       */
      getContext(): AudioContext {
        return this.audioContext;
      }

      /**
       * 取得 AnalyserNode 實例（供外部連接音訊源）
       */
      getAnalyser(): AnalyserNode {
        return this.analyser;
      }
    }
    ```
  - [ ] 實作音量計算輔助函式：
    ```typescript
    /**
     * 計算當前音量（0-1 範圍）
     * 使用時域數據（Time Domain）計算 RMS 音量
     */
    private calculateVolume(): number {
      // 取得時域數據
      this.analyser.getByteTimeDomainData(this.dataArray);

      // 計算 RMS (Root Mean Square) 音量
      let sum = 0;
      for (let i = 0; i < this.dataArray.length; i++) {
        const normalized = (this.dataArray[i] - 128) / 128; // 正規化到 -1~1
        sum += normalized * normalized;
      }
      const rms = Math.sqrt(sum / this.dataArray.length);

      return rms;
    }
    ```
  - [ ] 測試 AudioContext 與 AnalyserNode 正確初始化
  - [ ] 驗證音量計算函式返回 0-1 範圍數值

- [ ] **Task 3: 實作簡化 Viseme 映射邏輯** (AC: 3)
  - [ ] 實作音量到 Viseme 映射函式：
    ```typescript
    import type { VisemeType } from '@/types/lipsync';

    /**
     * 根據音量映射到對應的 Viseme 音素
     * POC 階段使用簡化映射策略（僅基於音量大小）
     *
     * @param volume - 當前音量（0-1 範圍）
     * @returns Viseme 音素類型與權重
     */
    private mapVolumeToViseme(volume: number): { viseme: VisemeType; weight: number } {
      const { volumeThresholds } = this.config;

      if (volume > volumeThresholds.high) {
        // 高音量 → 張嘴（aa）
        return {
          viseme: 'aa',
          weight: Math.min(volume, 1.0) // 音量即權重，但不超過 1.0
        };
      } else if (volume > volumeThresholds.medium) {
        // 中音量 → 半開（E）
        return {
          viseme: 'E',
          weight: (volume - volumeThresholds.medium) / (volumeThresholds.high - volumeThresholds.medium)
        };
      } else if (volume > volumeThresholds.low) {
        // 低音量 → 微開（neutral 過渡）
        return {
          viseme: 'neutral',
          weight: volume / volumeThresholds.medium
        };
      } else {
        // 極低音量 → 閉嘴
        return {
          viseme: 'neutral',
          weight: 0
        };
      }
    }
    ```
  - [ ] 加入映射邏輯註解說明策略
  - [ ] 測試不同音量值的映射結果：
    - [ ] volume = 0.8 → aa, weight ≈ 0.8
    - [ ] volume = 0.4 → E, weight ≈ 0.33
    - [ ] volume = 0.05 → neutral, weight = 0
  - [ ] 驗證權重值範圍在 0-1 之間

- [ ] **Task 4: 實作音訊分析主函式（產生 Viseme 時間軸）** (AC: 4, 5, 6)
  - [ ] 實作音訊緩衝區分析函式：
    ```typescript
    /**
     * 分析音訊緩衝區並產生 Viseme 時間軸數據
     *
     * @param audioBuffer - Web Audio API AudioBuffer
     * @returns Viseme 時間軸數據陣列
     */
    async analyseAudioBuffer(audioBuffer: AudioBuffer): Promise<VisemeData[]> {
      const visemeTimeline: VisemeData[] = [];
      const sampleIntervalSeconds = this.config.sampleInterval / 1000; // 轉換為秒
      const duration = audioBuffer.duration;

      // 建立離線音訊上下文（用於分析預錄音訊）
      const offlineContext = new OfflineAudioContext(
        audioBuffer.numberOfChannels,
        audioBuffer.length,
        audioBuffer.sampleRate
      );

      // 建立音訊源並連接到分析器
      const source = offlineContext.createBufferSource();
      source.buffer = audioBuffer;

      const offlineAnalyser = offlineContext.createAnalyser();
      offlineAnalyser.fftSize = this.config.fftSize;

      source.connect(offlineAnalyser);
      offlineAnalyser.connect(offlineContext.destination);

      // 按照取樣間隔分析音訊
      let currentTime = 0;
      const tempDataArray = new Uint8Array(offlineAnalyser.frequencyBinCount);

      while (currentTime < duration) {
        // 計算當前時間點的音量
        // 注意：OfflineAudioContext 需要使用 getByteTimeDomainData 即時獲取
        // POC 階段簡化：使用原始 AudioBuffer 數據計算音量
        const volume = this.calculateVolumeAtTime(audioBuffer, currentTime);

        // 映射到 Viseme
        const { viseme, weight } = this.mapVolumeToViseme(volume);

        // 加入時間軸數據
        visemeTimeline.push({
          time: currentTime,
          viseme,
          weight
        });

        currentTime += sampleIntervalSeconds;
      }

      console.log('[LipSync] Generated viseme timeline:', {
        totalDataPoints: visemeTimeline.length,
        duration: `${duration.toFixed(2)}s`,
        sampleInterval: `${this.config.sampleInterval}ms`
      });

      return visemeTimeline;
    }
    ```
  - [ ] 實作時間點音量計算輔助函式：
    ```typescript
    /**
     * 計算特定時間點的音量
     * POC 階段簡化實作：直接分析 AudioBuffer 原始數據
     *
     * @param audioBuffer - 音訊緩衝區
     * @param time - 時間點（秒）
     * @returns 音量值（0-1）
     */
    private calculateVolumeAtTime(audioBuffer: AudioBuffer, time: number): number {
      const sampleRate = audioBuffer.sampleRate;
      const channelData = audioBuffer.getChannelData(0); // 使用第一個聲道

      // 計算時間點對應的樣本索引
      const sampleIndex = Math.floor(time * sampleRate);

      // 取樣本周圍的數據計算 RMS 音量（使用 100 個樣本窗口）
      const windowSize = 100;
      const startIndex = Math.max(0, sampleIndex - windowSize / 2);
      const endIndex = Math.min(channelData.length, sampleIndex + windowSize / 2);

      let sum = 0;
      for (let i = startIndex; i < endIndex; i++) {
        sum += channelData[i] * channelData[i];
      }

      const rms = Math.sqrt(sum / (endIndex - startIndex));
      return Math.min(rms * 2, 1.0); // 放大 2 倍以增加敏感度，但不超過 1.0
    }
    ```
  - [ ] 加入 console.log 記錄分析進度
  - [ ] 測試音訊分析函式：
    - [ ] 輸入 3 秒音訊 → 產生約 30 個數據點（100ms 間隔）
    - [ ] 驗證時間軸數據結構正確
  - [ ] 驗證 visemeTimeline 陣列格式符合 VisemeData 介面

- [ ] **Task 5: 建立音訊載入與分析整合函式** (AC: 6)
  - [ ] 實作從 URL 載入音訊的輔助函式：
    ```typescript
    /**
     * 從 URL 載入音訊並解碼為 AudioBuffer
     *
     * @param audioUrl - 音訊檔案 URL（支援 MP3, WAV 等）
     * @returns AudioBuffer
     */
    async loadAudioFromUrl(audioUrl: string): Promise<AudioBuffer> {
      try {
        console.log('[LipSync] Loading audio from:', audioUrl);

        // Fetch 音訊檔案
        const response = await fetch(audioUrl);
        if (!response.ok) {
          throw new Error(`Failed to fetch audio: ${response.statusText}`);
        }

        // 轉換為 ArrayBuffer
        const arrayBuffer = await response.arrayBuffer();

        // 解碼為 AudioBuffer
        const audioBuffer = await this.audioContext.decodeAudioData(arrayBuffer);

        console.log('[LipSync] Audio loaded successfully:', {
          duration: `${audioBuffer.duration.toFixed(2)}s`,
          sampleRate: audioBuffer.sampleRate,
          channels: audioBuffer.numberOfChannels
        });

        return audioBuffer;
      } catch (error) {
        console.error('[LipSync] Failed to load audio:', error);
        throw error;
      }
    }
    ```
  - [ ] 建立整合函式（載入 + 分析）：
    ```typescript
    /**
     * 從 URL 載入音訊並分析產生 Viseme 時間軸
     * 這是對外的主要 API 函式
     *
     * @param audioUrl - 音訊檔案 URL
     * @returns Viseme 時間軸數據陣列
     */
    async analyseAudioFromUrl(audioUrl: string): Promise<VisemeData[]> {
      const audioBuffer = await this.loadAudioFromUrl(audioUrl);
      return this.analyseAudioBuffer(audioBuffer);
    }
    ```
  - [ ] 加入錯誤處理與日誌記錄
  - [ ] 測試從 URL 載入音訊並分析
  - [ ] 驗證完整流程無錯誤

- [ ] **Task 6: 建立測試腳本與主觀評估** (AC: 6, 7)
  - [ ] 建立測試目錄 `scripts/test/` (如尚未存在)
  - [ ] 建立 `scripts/test/test-lipsync-analysis.ts` 測試腳本：
    ```typescript
    /**
     * Lip Sync 音訊分析測試腳本
     * 用於驗證 viseme 數據生成功能
     *
     * 執行方式：
     * pnpm tsx scripts/test/test-lipsync-analysis.ts
     */

    import { AudioAnalyser } from '@/lib/three/lipsync';

    async function testAudioAnalysis() {
      console.log('=== Lip Sync Audio Analysis Test ===\n');

      // 建立分析器實例
      const analyser = new AudioAnalyser();

      // 測試音訊 URL（使用 Azure TTS 產生的範例音訊）
      // 注意：需要替換為實際可存取的音訊 URL
      const testAudioUrl = '/test-audio/sample.mp3';

      try {
        console.log('📂 Loading test audio...');
        const visemeTimeline = await analyser.analyseAudioFromUrl(testAudioUrl);

        console.log('\n✅ Viseme Timeline Generated:');
        console.log(`   Total data points: ${visemeTimeline.length}`);
        console.log(`   Duration: ${visemeTimeline[visemeTimeline.length - 1]?.time.toFixed(2)}s`);
        console.log(`   Sample interval: 100ms\n`);

        // 顯示前 10 個數據點
        console.log('📊 First 10 Viseme Data Points:');
        visemeTimeline.slice(0, 10).forEach((data, index) => {
          console.log(`   [${index}] time: ${data.time.toFixed(2)}s, viseme: ${data.viseme.padEnd(7)}, weight: ${data.weight.toFixed(2)}`);
        });

        // 統計 Viseme 分布
        const visemeCounts: Record<string, number> = {};
        visemeTimeline.forEach((data) => {
          visemeCounts[data.viseme] = (visemeCounts[data.viseme] || 0) + 1;
        });

        console.log('\n📈 Viseme Distribution:');
        Object.entries(visemeCounts).forEach(([viseme, count]) => {
          const percentage = ((count / visemeTimeline.length) * 100).toFixed(1);
          console.log(`   ${viseme.padEnd(7)}: ${count.toString().padStart(3)} (${percentage}%)`);
        });

        console.log('\n✅ Test completed successfully!');
        console.log('👁️  Please manually verify the viseme timeline matches audio content.');

      } catch (error) {
        console.error('\n❌ Test failed:', error);
      }
    }

    // 執行測試
    testAudioAnalysis();
    ```
  - [ ] 準備測試音訊檔案（3-5 秒繁中語音）
  - [ ] 執行測試腳本：`pnpm tsx scripts/test/test-lipsync-analysis.ts`
  - [ ] 驗證 console 輸出：
    - [ ] Viseme 時間軸數據正確產生
    - [ ] 資料點數量符合預期（duration / 0.1）
    - [ ] Viseme 類型分布合理（aa, E, neutral 都有出現）
  - [ ] 主觀視覺評估準備（Story 4.3 整合 Avatar 後進行）

- [ ] **Task 7: 優化與調校音量閾值** (AC: 7)
  - [ ] 測試多個音訊檔案並調整閾值
  - [ ] 實作閾值調校輔助函式：
    ```typescript
    /**
     * 調校音量閾值配置
     * 根據實際音訊特性調整最佳閾值
     *
     * @param high - 高音量閾值（預設 0.6）
     * @param medium - 中音量閾值（預設 0.3）
     * @param low - 低音量閾值（預設 0.1）
     */
    updateVolumeThresholds(high: number, medium: number, low: number): void {
      this.config.volumeThresholds = { high, medium, low };
      console.log('[LipSync] Volume thresholds updated:', this.config.volumeThresholds);
    }
    ```
  - [ ] 建立閾值測試矩陣：
    - [ ] 測試 high: 0.5, 0.6, 0.7
    - [ ] 測試 medium: 0.2, 0.3, 0.4
    - [ ] 測試 low: 0.05, 0.1, 0.15
  - [ ] 選擇最佳閾值組合（目標：70% 視覺匹配度）
  - [ ] 更新 DEFAULT_AUDIO_ANALYSIS_CONFIG 為最佳值
  - [ ] 記錄調校過程與結果於 Dev Notes

- [ ] **Task 8: 建立導出 API 與整合準備** (AC: 1, 4)
  - [ ] 在 `lib/three/lipsync.ts` 導出主要 API：
    ```typescript
    /**
     * 簡化 API：分析音訊 URL 並產生 Viseme 時間軸
     *
     * @param audioUrl - 音訊檔案 URL
     * @param config - 可選的分析配置
     * @returns Viseme 時間軸數據陣列
     */
    export async function generateVisemeTimeline(
      audioUrl: string,
      config?: Partial<AudioAnalysisConfig>
    ): Promise<VisemeData[]> {
      const analyser = new AudioAnalyser(config);
      return analyser.analyseAudioFromUrl(audioUrl);
    }

    // 導出類別供進階使用
    export { AudioAnalyser };
    ```
  - [ ] 建立 barrel export `lib/three/index.ts`：
    ```typescript
    export * from './lipsync';
    // 未來會加入其他 Three.js 相關模組
    ```
  - [ ] 更新 `types/index.ts` 導出 Lip Sync 型別：
    ```typescript
    export * from './lipsync';
    // 其他型別導出...
    ```
  - [ ] 驗證導入路徑正確：
    ```typescript
    // 其他組件可以這樣使用
    import { generateVisemeTimeline } from '@/lib/three';
    import type { VisemeData } from '@/types';
    ```
  - [ ] 建立 README 說明文件 `lib/three/README.md`：
    ```markdown
    # Lip Sync Library

    ## Usage

    ### Basic Usage
    \`\`\`typescript
    import { generateVisemeTimeline } from '@/lib/three';

    const audioUrl = '/audio/tts-output.mp3';
    const visemeTimeline = await generateVisemeTimeline(audioUrl);

    console.log(visemeTimeline);
    // [{ time: 0, viseme: 'neutral', weight: 0 }, ...]
    \`\`\`

    ### Advanced Usage
    \`\`\`typescript
    import { AudioAnalyser } from '@/lib/three';

    const analyser = new AudioAnalyser({
      sampleInterval: 50, // 50ms 更高頻率取樣
      volumeThresholds: {
        high: 0.7,
        medium: 0.4,
        low: 0.1
      }
    });

    const visemes = await analyser.analyseAudioFromUrl(audioUrl);
    \`\`\`
    ```
  - [ ] 驗證文件清晰易懂

---

## Dev Notes

### 相關來源樹（Source Tree）

根據架構文件，Epic 4 的檔案結構應如下：

```
avatar-chat-poc/
├── lib/
│   └── three/
│       ├── lipsync.ts              # Lip Sync 音訊分析主函式 (本 Story)
│       ├── animator.ts             # Avatar 動畫控制器 (Epic 2)
│       ├── avatar-loader.ts        # Avatar 模型載入器 (Epic 2)
│       ├── index.ts                # Barrel export
│       └── README.md               # Lip Sync 使用文件
├── types/
│   ├── lipsync.ts                  # Lip Sync 型別定義 (本 Story)
│   ├── avatar.ts                   # Avatar 型別定義 (Epic 2)
│   ├── chat.ts                     # Chat 型別定義 (Epic 3)
│   └── index.ts                    # 型別統一導出
├── components/
│   ├── avatar/
│   │   ├── AvatarModel.tsx         # Avatar 模型組件 (Epic 2)
│   │   └── LipSyncController.tsx   # Lip Sync 控制器組件 (Story 4.3)
│   └── chat/
│       └── ChatInterface.tsx       # 對話介面 (Epic 3)
├── stores/
│   ├── audioStore.ts               # Audio 狀態管理 (Epic 3)
│   └── avatarStore.ts              # Avatar 狀態管理 (Epic 2, 將加入 visemes)
├── scripts/
│   └── test/
│       └── test-lipsync-analysis.ts # Lip Sync 測試腳本 (本 Story)
└── public/
    └── test-audio/
        └── sample.mp3              # 測試音訊檔案
```

### 技術實作細節

**Web Audio API 核心概念**：
- **AudioContext**: 音訊處理環境，管理音訊節點圖
- **AnalyserNode**: 提供即時頻率與時域分析功能
- **AudioBuffer**: 解碼後的音訊數據，可用於離線分析
- **OfflineAudioContext**: 用於快速分析預錄音訊（非即時播放）

**音量計算方法（RMS）**：
```typescript
// RMS (Root Mean Square) 音量計算公式
// 1. 將音訊數據正規化到 -1 ~ 1 範圍
// 2. 計算平方和
// 3. 除以樣本數量
// 4. 開根號
const rms = Math.sqrt(sum / sampleCount);

// RMS 優點：
// - 反映音訊能量大小（比 peak 更穩定）
// - 符合人耳對音量的感知
// - 適合語音分析（平滑變化）
```

**Viseme 簡化映射策略**：

POC 階段使用基於音量的簡化映射（而非真實音素分析）：

| 音量範圍 | Viseme | 嘴型描述 | 適用場景 |
|---------|--------|---------|---------|
| > 0.6   | aa     | 張嘴（大開） | 高音量音節（如「啊」「哈」） |
| 0.3-0.6 | E      | 半開 | 中音量音節（如「欸」「誒」） |
| 0.1-0.3 | neutral | 微開 | 低音量音節（如「嗯」「呣」） |
| < 0.1   | neutral | 閉嘴 | 靜音或極低音量 |

**為何 POC 使用簡化策略？**
- ✅ 快速實現：無需複雜音素識別算法
- ✅ 成本低：無需額外 API（如 Azure Speech Viseme Output）
- ✅ 70% 準確度可達成：對於 POC 技術驗證足夠
- ⚠️ 限制：無法識別真實音素（如 p/b, f/v 差異）
- 🔄 未來優化：MVP 階段可升級為 Rhubarb Lip Sync 或 Azure Viseme API

**取樣間隔選擇（100ms）**：
```typescript
// 為何選擇 100ms？
// ✅ 足夠流暢：人眼無法察覺 < 100ms 的嘴型變化
// ✅ 效能平衡：每秒 10 個數據點，對 GPU 負擔小
// ✅ 符合語音節奏：中文音節持續時間通常 100-300ms
// ⚠️ 未來可調整：快速語音可降至 50ms，慢速語音可提升至 150ms

const sampleInterval = 100; // ms
const dataPointsPerSecond = 1000 / sampleInterval; // 10 points/sec
```

**AudioBuffer 離線分析原理**：
```typescript
// 為何使用離線分析而非即時分析？
// ✅ 預先計算：TTS 音訊已完整產生，可提前分析
// ✅ 準確度高：無需處理即時串流的時間對齊問題
// ✅ 效能更好：一次性分析完成，不佔用播放時 CPU
// ✅ 可重複使用：viseme 時間軸可快取，重複播放無需重新分析

// 流程：
// 1. Fetch 音訊 URL → ArrayBuffer
// 2. decodeAudioData → AudioBuffer
// 3. 遍歷每個取樣點計算音量
// 4. 映射到 Viseme → 時間軸數據
// 5. 儲存至 audioStore 供播放時使用
```

### 重要架構決策

1. **為何建立 AudioAnalyser 類別而非純函式？**
   - ✅ 封裝狀態：AudioContext, AnalyserNode 需要重複使用
   - ✅ 配置彈性：可透過 constructor 自訂分析參數
   - ✅ 未來擴展：可加入快取、事件監聽等功能
   - ✅ 測試友善：可 mock 類別實例進行單元測試
   - ⚠️ 注意：需要手動釋放 AudioContext 資源（dispose 方法）

2. **為何不使用 Azure Speech Viseme Output？**
   - ✅ 成本考量：Azure Viseme API 需額外費用
   - ✅ 延遲考量：需額外 API 請求增加延遲
   - ✅ POC 目標：70% 準確度即可驗證可行性
   - ✅ 簡化開發：Web Audio API 原生支援，無需外部依賴
   - 🔄 未來升級路徑：MVP 階段可評估 Azure Viseme 或 Rhubarb Lip Sync

3. **為何每 100ms 取樣而非連續分析？**
   - ✅ 效能平衡：連續分析每幀（60fps）產生 3600 個數據點/分鐘，過於龐大
   - ✅ 視覺平滑：100ms 間隔足以產生流暢動畫（人眼感知閾值 ~80ms）
   - ✅ 數據量控制：1 分鐘音訊 → 600 個數據點，可接受
   - ✅ 插值友善：Story 4.3 可使用 lerp 在數據點間插值

4. **為何使用 RMS 而非 Peak 音量？**
   - ✅ 平滑穩定：RMS 反映平均能量，不受瞬間尖峰影響
   - ✅ 符合人耳：人耳對音量的感知更接近 RMS 而非 Peak
   - ✅ 適合語音：語音音量變化相對平緩，RMS 更能反映實際響度
   - ⚠️ 注意：音樂分析可能需要 Peak，但本專案僅處理 TTS 語音

5. **為何定義 VisemeType 為 string literal union？**
   - ✅ 型別安全：TypeScript 編譯時檢查，避免拼寫錯誤
   - ✅ IDE 支援：自動完成提示可用的 Viseme 類型
   - ✅ 可擴展：未來加入更多音素（如 'p', 'f', 'm'）只需更新型別
   - ✅ 文檔清晰：型別定義即文檔，開發者一目了然

### Testing

**測試框架**: 手動測試 + TypeScript 編譯檢查 + 測試腳本（POC 階段）

**測試範圍**:
1. ✅ AudioAnalyser 類別正確初始化
2. ✅ 音量計算函式返回 0-1 範圍
3. ✅ Viseme 映射邏輯正確（不同音量對應正確音素）
4. ✅ 音訊載入與解碼無錯誤
5. ✅ Viseme 時間軸數據結構正確
6. ✅ 取樣間隔符合 100ms 要求
7. ✅ console.log 輸出清晰易讀
8. ✅ 主觀視覺評估（Story 4.3 整合後進行）

**測試執行方式**:
```bash
# 1. 編譯 TypeScript 檢查型別錯誤
pnpm build

# 2. 執行測試腳本
pnpm tsx scripts/test/test-lipsync-analysis.ts

# 預期輸出：
# === Lip Sync Audio Analysis Test ===
#
# 📂 Loading test audio...
# [LipSync] Loading audio from: /test-audio/sample.mp3
# [LipSync] Audio loaded successfully: { duration: 3.24s, sampleRate: 22050, channels: 1 }
# [LipSync] Generated viseme timeline: { totalDataPoints: 32, duration: 3.20s, sampleInterval: 100ms }
#
# ✅ Viseme Timeline Generated:
#    Total data points: 32
#    Duration: 3.20s
#    Sample interval: 100ms
#
# 📊 First 10 Viseme Data Points:
#    [0] time: 0.00s, viseme: neutral, weight: 0.00
#    [1] time: 0.10s, viseme: E,       weight: 0.42
#    [2] time: 0.20s, viseme: aa,      weight: 0.78
#    ...
#
# 📈 Viseme Distribution:
#    neutral: 15 (46.9%)
#    E:       10 (31.2%)
#    aa:       7 (21.9%)
#
# ✅ Test completed successfully!
# 👁️  Please manually verify the viseme timeline matches audio content.
```

**驗證清單**:
- [ ] TypeScript 編譯無錯誤（`pnpm build`）
- [ ] 測試腳本正常執行無崩潰
- [ ] Viseme 時間軸數據點數量正確（duration / 0.1）
- [ ] Viseme 類型分布合理（neutral, E, aa 都有出現）
- [ ] 音量映射邏輯正確（高音量 → aa, 中音量 → E）
- [ ] console.log 輸出清晰且資訊完整
- [ ] 可匯入並使用 `generateVisemeTimeline` 函式
- [ ] 型別定義可正確導入（`import type { VisemeData } from '@/types'`）

### 效能考量

**記憶體使用**:
- **AudioBuffer**: 每秒音訊約 44KB（22050Hz, Mono, 16-bit）
- **VisemeData 陣列**: 每分鐘音訊約 600 個數據點 × 24 bytes ≈ 14.4 KB
- **總記憶體**: 1 分鐘音訊 < 3 MB（AudioBuffer + Viseme Data）
- ✅ 可接受範圍（POC 目標 < 500 MB 總記憶體）

**分析效能**:
- **離線分析時間**: 3 秒音訊約需 50-100ms 分析時間
- **瓶頸**: `decodeAudioData` 解碼時間（約佔 80%）
- **優化策略**:
  - 使用低位元率 TTS 音訊（16kbps MP3）減少解碼時間
  - 快取已分析的 viseme 時間軸（避免重複分析）

**未來優化方向**（MVP 階段）:
1. **Web Worker 分析**: 將音訊分析移至 Worker 避免阻塞主執行緒
2. **音訊快取**: localStorage 快取常用語句的 viseme 數據
3. **動態取樣**: 根據語速自動調整取樣間隔（快速語音 50ms, 慢速 150ms）
4. **GPU 加速**: 使用 WebGL Shader 加速音量計算（實驗性）

### 安全性考量

**音訊來源驗證**:
- ✅ 僅接受內部 TTS API 產生的音訊（/api/tts 返回）
- ⚠️ 若開放外部 URL，需檢查 CORS 與內容安全策略
- ✅ 使用 `try-catch` 捕捉載入錯誤，避免崩潰

**資源釋放**:
```typescript
// AudioContext 需手動關閉避免記憶體洩漏
export class AudioAnalyser {
  // ... existing code ...

  /**
   * 釋放 AudioContext 資源
   * 使用完畢後必須呼叫此方法
   */
  dispose(): void {
    this.audioContext.close();
    console.log('[LipSync] AudioContext disposed');
  }
}
```

**錯誤處理**:
- ✅ 所有 async 函式使用 try-catch
- ✅ 載入失敗時拋出明確錯誤訊息
- ✅ console.error 記錄錯誤詳情（開發模式）
- 🔄 Story 4.5 將實作降級方案（分析失敗時跳過 Lip Sync）

### 依賴關係

**前置條件**:
- ✅ Story 1.1 已完成（Next.js 專案已建立）
- ✅ Story 3.5 已完成（TTS API 可產生音訊檔）
- ✅ Story 3.6 已完成（Web Audio API 音訊播放）
- ✅ TypeScript 與 ESLint 已配置

**後續 Story 依賴**:
- **Story 4.2**: 會使用本 Story 的 VisemeData 型別定義
- **Story 4.3**: 會使用 `generateVisemeTimeline` 函式產生數據
- **Story 4.4**: 會調整本 Story 的音量閾值與映射邏輯
- **Story 4.5**: 會為本 Story 加入錯誤處理與降級方案

**關鍵路徑**:
- 本 Story 是 Epic 4 的基礎，必須優先完成
- Story 4.2（Blendshape 控制）依賴本 Story 的 Viseme 型別
- Story 4.3（Lip Sync Controller）依賴本 Story 的分析函式

**與 Epic 3 整合**:
- 本 Story 使用 Epic 3 的 TTS API 產生的音訊
- audioStore（Epic 3）未來會加入 visemes 狀態（Story 4.3）
- 完整流程：Chat API → TTS API → 音訊分析 → Viseme 數據 → Avatar 嘴型

**外部依賴**:
- **Web Audio API**: 瀏覽器原生 API（Chrome/Edge/Safari 支援）
- **TypeScript**: 型別定義與編譯檢查
- **無外部套件**: 本 Story 不引入任何 npm 套件（使用原生 API）

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-14 | 1.0 | 建立 Story 4.1 草稿 | SM Agent |

---

## Dev Agent Record

*(此部分由 Dev Agent 在實作時填寫)*

### Agent Model Used
_待填寫_

### Debug Log References
_待填寫_

### Completion Notes List
_待填寫_

### File List
_待填寫_

---

## QA Results

*(此部分由 QA Agent 在審核時填寫)*

_待填寫_
